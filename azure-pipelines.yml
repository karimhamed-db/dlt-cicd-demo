# Python package
# Create and test a Python package on multiple Python versions.
# Add steps that analyze code, save the dist with the build record, publish to a PyPI-compatible index, and more:
# https://docs.microsoft.com/azure/devops/pipelines/languages/python

trigger:
- azure/main

pool:
  vmImage: ubuntu-latest

steps:
- task: UsePythonVersion@0
  inputs:
    versionSpec: 3.8
  displayName: 'Use Python 3.8'

- checkout: self
  persistCredentials: true
  clean: true

- script: git checkout azure/main
  displayName: 'Get latest branch'

- script: |
    python -m pip install --upgrade pip
    pip install -r unit-requirements.txt
  displayName: 'Install dependencies'

- script: |
    databricks repos update --path $REPO_LOCATION --branch azure/main
  displayName: 'Update notebooks'
  env:
    DATABRICKS_TOKEN: $(DATABRICKS_TOKEN) 

- script: |
    for PIPELINE_DIRECTORY in ./pipelines/*
    do
      if [ -d "$PIPELINE_DIRECTORY" ]
      then
        PIPELINE_NAME=$(basename $PIPELINE_DIRECTORY)
        echo "Deploying $PIPELINE_NAME"

        echo "Update $PIPELINE_NAME pipeline location configuration to $REPO_LOCATION/pipelines/$PIPELINE_NAME"
        sed -i "s|{{location_prefix}}|$REPO_LOCATION/pipelines/$PIPELINE_NAME|" $PIPELINE_DIRECTORY/pipeline.json

        pipeline_id=$(databricks pipelines list | jq -r --arg pipeline_name "$PIPELINE_NAME" '.[] | select(.name==$pipeline_name) | .pipeline_id')
        if [ -z "$pipeline_id" ]
        then
          echo "Creating $PIPELINE_NAME Pipeline"
          databricks pipelines create --settings $PIPELINE_DIRECTORY/pipeline.json
        else
          echo "Updating $PIPELINE_NAME Pipeline"
          databricks pipelines edit --pipeline-id $pipeline_id --settings $PIPELINE_DIRECTORY/pipeline.json
        fi
      fi
    done
  displayName: 'Deploy DLT pipelines'
  env:
    DATABRICKS_TOKEN: $(DATABRICKS_TOKEN) 